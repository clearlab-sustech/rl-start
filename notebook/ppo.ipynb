{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.rl_utils as rl_utils\n",
    "\n",
    "from alg.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation space**: The observation is a 4-dimensional vector $\\mathcal{S} \\in \\mathbb{R}^4$, where each element represents:\n",
    "- Cart position: $x \\in [-4.8, 4.8]$\n",
    "- Cart velocity: $\\dot{x} \\in (-\\infty, \\infty)$\n",
    "- Pole angle: $\\theta \\in [-24^\\circ, 24^\\circ]$\n",
    "- Pole angular velocity: $\\dot{\\theta} \\in (-\\infty, \\infty)$\n",
    "\n",
    "**Action space**: $a \\in \\{0, 1\\}$, indicating the direction to apply force on the cart. \n",
    "- $0$: Apply force to the left.\n",
    "- $1$: Apply force to the right.\n",
    "\n",
    "**Initial state**: The episode starts with the cart at the center of the track ($x = 0$), and the pole almost upright with small random values for the pole angle and velocity.\n",
    "\n",
    "**Termination of an episode**:\n",
    "- Case 1: The pole's angle $\\theta$ exceeds $24^\\circ$ from vertical.\n",
    "- Case 2: The cart's position $x$ exceeds $4.8$ units from the center.\n",
    "- Case 3: (Truncation) The episode reaches its maximum length of 200 timesteps.\n",
    "\n",
    "**Reward**: A reward of $+1$ is provided for every timestep the pole remains balanced, until one of the termination conditions is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cart_pole.gif\" alt=\"cart_pole_env\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "num_episodes = 500\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "epochs = 10\n",
    "eps = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device)\n",
    "\n",
    "return_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)\n",
    "\n",
    "# Iteration 0: 100%|██████████| 50/50 [00:10<00:00,  4.81it/s, episode=50,\n",
    "# return=183.200]\n",
    "# Iteration 1: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, episode=100,\n",
    "# return=191.400]\n",
    "# Iteration 2: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, episode=150,\n",
    "# return=199.900]\n",
    "# Iteration 3: 100%|██████████| 50/50 [00:21<00:00,  2.33it/s, episode=200,\n",
    "# return=200.000]\n",
    "# Iteration 4: 100%|██████████| 50/50 [00:21<00:00,  2.29it/s, episode=250,\n",
    "# return=200.000]\n",
    "# Iteration 5: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, episode=300,\n",
    "# return=200.000]\n",
    "# Iteration 6: 100%|██████████| 50/50 [00:23<00:00,  2.14it/s, episode=350,\n",
    "# return=200.000]\n",
    "# Iteration 7: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, episode=400,\n",
    "# return=200.000]\n",
    "# Iteration 8: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, episode=450,\n",
    "# return=200.000]\n",
    "# Iteration 9: 100%|██████████| 50/50 [00:22<00:00,  2.25it/s, episode=500,\n",
    "# return=200.000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('PPO on {}'.format(env_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ppo_cartpole](./images/ppo_cartpole.png)\n",
    "![ac_cartpole](./images/ac_cartpole.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_policy(agent, env, num_episodes=5):\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_return = 0\n",
    "        while not done:\n",
    "            env.render()  \n",
    "            action = agent.take_action(state)  \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        print(f\"Episode {i + 1}, Return: {episode_return}\")\n",
    "    \n",
    "    env.close()  # 渲染完成后关闭环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_trained_policy(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: PPO Training on the Pendulum-v1 Environment\n",
    "\n",
    "## Objective:\n",
    "Your task is to implement and train a Proximal Policy Optimization (PPO) agent on the `Pendulum-v1` environment using PyTorch. The goal is to learn a policy that balances the pendulum upright in continuous action space.\n",
    "\n",
    "## Requirements:\n",
    "1. **Environment**: Use the `Pendulum-v1` environment from `gym`, which has a **continuous action space**. \n",
    "   - Observation space: A 3-dimensional vector representing the cosine and sine of the pendulum angle and the angular velocity.\n",
    "   - Action space: A single continuous value in the range $[-2, 2]$ representing the torque applied to the pendulum.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Implement a **policy network** (actor) that outputs the mean of a Gaussian distribution for the continuous action space.\n",
    "   - Implement a **value network** (critic) to estimate the state value.\n",
    "\n",
    "3. **Training**:\n",
    "   - Use the **PPO algorithm** to train your agent.\n",
    "   - Use the clipped objective function for PPO.\n",
    "   - Train for at least **500 episodes**.\n",
    "\n",
    "4. **Testing**:\n",
    "   - After training, **test the agent** on the environment for 5 episodes and render the environment during testing to visualize the results.\n",
    "\n",
    "5. **Deliverables**:\n",
    "   - Submit your Jupyter Notebook containing:\n",
    "     - The PPO agent implementation.\n",
    "     - Training loop.\n",
    "     - Plots of the **total reward per episode** during training.\n",
    "     - Testing the trained agent with rendered results.\n",
    "\n",
    "## Extra Credit:\n",
    "- Implement **entropy regularization** to encourage exploration during training.\n",
    "- Compare the performance of the PPO agent with and without entropy regularization.\n",
    "\n",
    "## Submission Guidelines:\n",
    "- Submit your completed Jupyter Notebook via the provided platform by the deadline.\n",
    "- Ensure your code is well-documented, with comments explaining key steps in your implementation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38-pytorch_1.7.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

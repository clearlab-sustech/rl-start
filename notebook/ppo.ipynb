{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Proximal Policy Optimization (PPO) Implementation Tutorial\n",
                "\n",
                "This notebook provides a step-by-step implementation of the Proximal Policy Optimization (PPO) algorithm, tested on the CartPole environment. PPO is a policy gradient method that has become one of the most popular reinforcement learning algorithms due to its simplicity and effectiveness.\n",
                "\n",
                "## Key Features of PPO\n",
                "1. **Trust Region Update**: Uses a clipped objective function to prevent too large policy updates\n",
                "2. **Actor-Critic Architecture**: Combines value function estimation with policy optimization\n",
                "3. **GAE**: Implements Generalized Advantage Estimation for more stable training\n",
                "\n",
                "Let's start by importing the required libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "\n",
                "import gymnasium as gym\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from utils import rl_utils"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment Setup\n",
                "\n",
                "We'll be using the CartPole-v1 environment from OpenAI Gym. This is a classic control problem where the agent needs to balance a pole on a moving cart."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env_name = \"CartPole-v0\"\n",
                "env = gym.make(env_name)\n",
                "seed = 42  # Choose any integer\n",
                "rl_utils.set_seed(env, seed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment Details\n",
                "\n",
                "**Observation Space**: A 4-dimensional vector representing:\n",
                "- Cart Position: $x \\in [-4.8, 4.8]$\n",
                "- Cart Velocity: $\\dot{x} \\in (-\\infty, \\infty)$\n",
                "- Pole Angle: $\\theta \\in [-24^\\circ, 24^\\circ]$\n",
                "- Pole Angular Velocity: $\\dot{\\theta} \\in (-\\infty, \\infty)$\n",
                "\n",
                "**Action Space**: Binary choice:\n",
                "- 0: Push cart left\n",
                "- 1: Push cart right\n",
                "\n",
                "**Reward**: +1 for each timestep the pole remains upright\n",
                "\n",
                "## Neural Network Architecture\n",
                "\n",
                "We'll implement both the policy network (actor) and value network (critic) using simple feedforward neural networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PolicyNet(torch.nn.Module):\n",
                "    \"\"\"Actor network that predicts action probabilities\"\"\"\n",
                "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
                "        super(PolicyNet, self).__init__()\n",
                "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
                "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.fc1(x))\n",
                "        return F.softmax(self.fc2(x), dim=1)\n",
                "\n",
                "class ValueNet(torch.nn.Module):\n",
                "    \"\"\"Critic network that estimates state values\"\"\"\n",
                "    def __init__(self, state_dim, hidden_dim):\n",
                "        super(ValueNet, self).__init__()\n",
                "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
                "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.fc1(x))\n",
                "        return self.fc2(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advantage Estimation\n",
                "\n",
                "PPO uses Generalized Advantage Estimation (GAE) to compute advantages. This helps reduce variance while maintaining an acceptable level of bias in our policy gradient estimates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_advantage(gamma, lmbda, td_delta):\n",
                "    \"\"\"Compute advantage using GAE (Generalized Advantage Estimation)\n",
                "    \n",
                "    Args:\n",
                "        gamma: Discount factor\n",
                "        lmbda: GAE parameter\n",
                "        td_delta: Temporal difference error\n",
                "    \"\"\"\n",
                "    td_delta = td_delta.detach().numpy()\n",
                "    advantage_list = []\n",
                "    advantage = 0.0\n",
                "    for delta in td_delta[::-1]:\n",
                "        advantage = gamma * lmbda * advantage + delta\n",
                "        advantage_list.append(advantage)\n",
                "    advantage_list.reverse()\n",
                "    return torch.tensor(advantage_list, dtype=torch.float)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PPO Agent Implementation\n",
                "\n",
                "Now we'll implement the main PPO agent class that combines all the components:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PPO:\n",
                "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
                "                 lmbda, epochs, eps, gamma, device):\n",
                "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
                "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
                "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
                "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
                "        \n",
                "        self.gamma = gamma  # Discount factor\n",
                "        self.lmbda = lmbda  # GAE parameter\n",
                "        self.epochs = epochs  # Number of epochs to update the policy\n",
                "        self.eps = eps  # Clip parameter for PPO\n",
                "        self.device = device\n",
                "\n",
                "    def take_action(self, state, test=False):\n",
                "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
                "        probs = self.actor(state)\n",
                "        if test:  # During testing, choose the most probable action\n",
                "            action = torch.argmax(probs, dim=1)\n",
                "        else:  # During training, sample from the action distribution\n",
                "            action_dist = torch.distributions.Categorical(probs)\n",
                "            action = action_dist.sample()\n",
                "        return action.item()\n",
                "\n",
                "    def update(self, transition_dict):\n",
                "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
                "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
                "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
                "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
                "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
                "        \n",
                "        # Compute TD target and advantage\n",
                "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)\n",
                "        td_delta = td_target - self.critic(states)\n",
                "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
                "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
                "        \n",
                "        # PPO update for multiple epochs\n",
                "        for _ in range(self.epochs):\n",
                "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
                "            ratio = torch.exp(log_probs - old_log_probs)\n",
                "            surr1 = ratio * advantage\n",
                "            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage\n",
                "            \n",
                "            # Update actor and critic networks\n",
                "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
                "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
                "            \n",
                "            self.actor_optimizer.zero_grad()\n",
                "            self.critic_optimizer.zero_grad()\n",
                "            actor_loss.backward()\n",
                "            critic_loss.backward()\n",
                "            self.actor_optimizer.step()\n",
                "            self.critic_optimizer.step()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Functions\n",
                "\n",
                "Let's implement the training loop to collect experience and update the policy:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_episode(env, agent):\n",
                "    \"\"\"Train for one episode\"\"\"\n",
                "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
                "    state = env.reset()\n",
                "    if isinstance(state, tuple):  # Handle new gym API\n",
                "        state = state[0]\n",
                "    episode_return = 0\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        action = agent.take_action(state)\n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        done = terminated or truncated\n",
                "\n",
                "        # Store transition\n",
                "        states.append(state)\n",
                "        actions.append(action)\n",
                "        rewards.append(reward)\n",
                "        next_states.append(next_state)\n",
                "        dones.append(done)\n",
                "\n",
                "        state = next_state\n",
                "        episode_return += reward\n",
                "\n",
                "    # Update policy using collected transitions\n",
                "    transition_dict = {\n",
                "        'states': np.array(states),\n",
                "        'actions': np.array(actions),\n",
                "        'rewards': np.array(rewards),\n",
                "        'next_states': np.array(next_states),\n",
                "        'dones': np.array(dones)\n",
                "    }\n",
                "    agent.update(transition_dict)\n",
                "    return episode_return\n",
                "\n",
                "def train(env, agent, num_episodes):\n",
                "    \"\"\"Complete training process\"\"\"\n",
                "    return_list = []\n",
                "    for i in range(num_episodes):\n",
                "        episode_return = train_episode(env, agent)\n",
                "        return_list.append(episode_return)\n",
                "        if (i + 1) % 10 == 0:\n",
                "            print(f\"Episode {i+1}, Return: {episode_return}\")\n",
                "    return return_list"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the Agent\n",
                "\n",
                "Now let's set up the hyperparameters and train our PPO agent:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "state_dim = env.observation_space.shape[0]\n",
                "action_dim = env.action_space.n\n",
                "hidden_dim = 128\n",
                "actor_lr = 1e-3\n",
                "critic_lr = 1e-2\n",
                "gamma = 0.98\n",
                "lmbda = 0.95\n",
                "epochs = 10\n",
                "eps = 0.2\n",
                "num_episodes = 500\n",
                "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
                "\n",
                "# Create and train agent\n",
                "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
                "           lmbda, epochs, eps, gamma, device)\n",
                "return_list = train(env, agent, num_episodes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing the Results\n",
                "\n",
                "Let's plot the training curve using a moving average to smooth out the noise:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training returns\n",
                "episodes_list = list(range(len(return_list)))\n",
                "mv_return = rl_utils.moving_average(return_list, 9)\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(episodes_list, mv_return)\n",
                "plt.xlabel('Episodes')\n",
                "plt.ylabel('Returns')\n",
                "plt.title(f'PPO Learning Curve on {env_name}')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing the Trained Agent\n",
                "\n",
                "Now that we have trained our agent, let's create a function to visualize its performance. This function will:\n",
                "1. Run multiple test episodes\n",
                "2. Render each episode\n",
                "3. Display the cumulative reward\n",
                "\n",
                "Note: We'll use `test=True` in the `take_action` method to make the agent choose actions deterministically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython import display\n",
                "import PIL.Image\n",
                "\n",
                "def test_trained_policy(agent, env, num_episodes=5):\n",
                "    \"\"\"Test the trained policy and visualize the agent's behavior\n",
                "    \n",
                "    Args:\n",
                "        agent: Trained PPO agent\n",
                "        env: Gym environment\n",
                "        num_episodes: Number of test episodes to run\n",
                "    \"\"\"\n",
                "    for i in range(num_episodes):\n",
                "        state = env.reset()\n",
                "        if isinstance(state, tuple):\n",
                "            state = state[0]\n",
                "        done = False\n",
                "        episode_return = 0\n",
                "        step_count = 0\n",
                "\n",
                "        print(f\"\\nEpisode {i + 1}:\")\n",
                "        while not done:\n",
                "            # Render the environment\n",
                "            rgb_array = env.render()\n",
                "            img = PIL.Image.fromarray(rgb_array)\n",
                "            display.clear_output(wait=True)\n",
                "            display.display(img)\n",
                "            \n",
                "            # Take deterministic action\n",
                "            action = agent.take_action(state, test=True)\n",
                "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "            done = terminated or truncated\n",
                "            \n",
                "            state = next_state\n",
                "            episode_return += reward\n",
                "            step_count += 1\n",
                "            \n",
                "            # Print current status\n",
                "            print(f\"Step: {step_count}, Cumulative Reward: {episode_return}\")\n",
                "\n",
                "        print(f\"\\nEpisode {i + 1} finished with total return: {episode_return}\")\n",
                "\n",
                "    env.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's test our trained agent:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a new environment instance for testing with rendering enabled\n",
                "test_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
                "test_trained_policy(agent, test_env)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis and Discussion\n",
                "\n",
                "Let's analyze the key components of our PPO implementation:\n",
                "\n",
                "1. **Policy Network (Actor)**:\n",
                "   - Maps states to action probabilities\n",
                "   - Uses a simple two-layer neural network\n",
                "   - Outputs softmax probabilities for discrete actions\n",
                "\n",
                "2. **Value Network (Critic)**:\n",
                "   - Estimates the value function V(s)\n",
                "   - Helps reduce variance in policy updates\n",
                "   - Also uses a two-layer architecture\n",
                "\n",
                "3. **PPO Clipping**:\n",
                "   - Prevents too large policy updates\n",
                "   - Clip parameter ε=0.2 is a common choice\n",
                "   - Helps maintain stable learning\n",
                "\n",
                "4. **Advantage Estimation**:\n",
                "   - Uses GAE for better trade-off between bias and variance\n",
                "   - Lambda parameter controls this trade-off\n",
                "\n",
                "## Potential Improvements\n",
                "\n",
                "1. **Network Architecture**:\n",
                "   - Add more layers or units\n",
                "   - Try different activation functions\n",
                "\n",
                "2. **Training Process**:\n",
                "   - Implement parallel environment sampling\n",
                "   - Add entropy bonus for exploration\n",
                "   - Try different learning rate schedules\n",
                "\n",
                "3. **Hyperparameter Tuning**:\n",
                "   - Adjust GAE parameters (γ, λ)\n",
                "   - Modify network learning rates\n",
                "   - Change the number of epochs and batch size\n",
                "\n",
                "## Conclusion\n",
                "\n",
                "We've successfully implemented and trained a PPO agent to solve the CartPole environment. The implementation includes all key components of the PPO algorithm while maintaining simplicity and readability. The training results show that our agent can learn to balance the pole effectively, demonstrating the power of PPO even with a relatively simple implementation."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
